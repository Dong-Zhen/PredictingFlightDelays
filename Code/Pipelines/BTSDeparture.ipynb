{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Depart and Delay Data for Delta Flights from Atl, LA, CHI, DAL (FT Worth), Den, NY (JFK), SF, Seattle (Tacona), McCarran (LV) airports for 2019.\n",
    "\n",
    "Data File Path: r'/home/desbrium/Metis/PredictingFlightDelays/Data/BTS Departure Data'\n",
    "\n",
    "Final Pickle: r'/home/desbrium/Metis/PredictingFlightDelays/Data/BTS Departure Data/bts_10_airport_departures2019.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver = \"/usr/local/bin/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_usairports(selection=None, download_path = r'/home/desbrium/Metis/PredictingFlightDelays/Data/BTS Departure Data'):\n",
    "    \n",
    "    file_path = os.path.join(download_path,f'{selection} airports.csv')\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "    \n",
    "        usairports_url = \"https://en.wikipedia.org/wiki/List_of_airports_in_the_United_States\"\n",
    "\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "        driver.get(usairports_url)\n",
    "\n",
    "        driver.execute_script((\"window.scrollBy(0,700)\"))\n",
    "\n",
    "        xpath = \"//*[@id='mw-content-text']/div[1]/table[2]/thead/tr/th[7]\"\n",
    "\n",
    "        sort_enplanement = driver.find_element_by_xpath(xpath)\n",
    "\n",
    "        for click in range(2):\n",
    "\n",
    "            sort_enplanement.click()\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        airport_data_list = []\n",
    "\n",
    "        airport_table = soup.find('table',class_ = 'wikitable sortable jquery-tablesorter')\n",
    "\n",
    "        headers = airport_table.find_all('th')\n",
    "        airport_columns = [header.text.replace(\"\\n\", \"\") for header in headers]\n",
    "\n",
    "        airports = airport_table.find_all('tr')[1:selection + 1]\n",
    "\n",
    "        for airport in airports:\n",
    "\n",
    "            td = airport.find_all('td')\n",
    "            airport_data_list.append([data.text.replace(\"\\n\", \"\") for data in td])\n",
    "\n",
    "        airport_df = pd.DataFrame(airport_data_list, columns = airport_columns)\n",
    "\n",
    "        airport_df.to_csv(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bts_data(selection, year, headless=True, download_path = r'/home/desbrium/Metis/PredictingFlightDelays/Data/BTS Departure Data'):\n",
    "    \n",
    "    option = webdriver.ChromeOptions()\n",
    "    \n",
    "    #if headless:\n",
    "        #option.add_argument('headless')\n",
    "    \n",
    "    if download_path is not None:\n",
    "    \n",
    "        prefs = {}\n",
    "        os.makedirs(download_path, exist_ok=True)\n",
    "        prefs[\"profile.default_content_settings.popups\"]=0\n",
    "        prefs[\"download.default_directory\"]=download_path\n",
    "        option.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(chromedriver, options = option)\n",
    "    \n",
    "    bts_website = \"https://transtats.bts.gov/ONTIME/Departures.aspx\"\n",
    "    driver.get(bts_website)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    airports = [airport.get('value') for airport in soup.find_all('option')]    \n",
    "    \n",
    "    selected_airports_df = pd.read_csv(extract_usairports(selection))\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    airport_FAA_names = list(selected_airports_df['FAA'])\n",
    "       \n",
    "    time.sleep(10)\n",
    "    \n",
    "    for i, airport in enumerate(airport_FAA_names):\n",
    "        \n",
    "        file_path = os.path.join(download_path,f'{airport}{year}.csv')\n",
    "            \n",
    "        try:\n",
    "            if airport in airports and i == 0:\n",
    "\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "\n",
    "                select_airport = driver.find_element_by_xpath(f\"//select[@id='cboAirport']/option[@value='{airport}']\")\n",
    "                select_airport.click()\n",
    "\n",
    "                airline = 'DL' #Requires shortform name of airline\n",
    "                select_airline = driver.find_element_by_xpath(f\"//select[@id='cboAirline']/option[@value='{airline}']\")\n",
    "                select_airline.click()\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "                required_fields = ['Statistics','Months','Days', year]\n",
    "\n",
    "                for field in required_fields:\n",
    "\n",
    "                    if field == year:\n",
    "\n",
    "                        check_box = driver.find_element_by_xpath(f\"//input[@type='checkbox' and @value={year}]\")\n",
    "                        check_box.click()\n",
    "\n",
    "                        time.sleep(3)\n",
    "\n",
    "                    else:   \n",
    "\n",
    "                        check_box = driver.find_element_by_xpath(f\"//input[@id='chkAll{field}']\")\n",
    "                        check_box.click()\n",
    "                        \n",
    "                        time.sleep(3)\n",
    "\n",
    "                submit = driver.find_element_by_xpath(f\"//input[@name='btnSubmit' and @value='Submit']\")\n",
    "                submit.click()\n",
    "\n",
    "                time.sleep(15)\n",
    "\n",
    "                download = driver.find_element_by_xpath(f\"//a[@id='DL_CSV']\")\n",
    "                download.click()\n",
    "\n",
    "                time.sleep(30)\n",
    "\n",
    "                bts_df = pd.read_csv(os.path.join(download_path,'Detailed_Statistics_Departures.csv'), header = 6, skipfooter= 1, engine = 'python')\n",
    "                bts_df['Airport'] = airport\n",
    "\n",
    "                os.remove(os.path.join(download_path,'Detailed_Statistics_Departures.csv')) \n",
    "\n",
    "                bts_df.to_csv(file_path)\n",
    "\n",
    "                yield file_path\n",
    "\n",
    "            elif airport in airports and i != 0:\n",
    "\n",
    "                if not os.path.isfile(file_path):\n",
    "\n",
    "                    select_airport = driver.find_element_by_xpath(f\"//select[@id='cboAirport']/option[@value='{airport}']\")\n",
    "                    select_airport.click()\n",
    "\n",
    "                    submit = driver.find_element_by_xpath(f\"//input[@name='btnSubmit' and @value='Submit']\")\n",
    "                    submit.click()\n",
    "\n",
    "                    time.sleep(10)\n",
    "\n",
    "                    download = driver.find_element_by_xpath(f\"//a[@id='DL_CSV']\")\n",
    "                    download.click()\n",
    "\n",
    "                    time.sleep(10)\n",
    "\n",
    "                    bts_df = pd.read_csv(os.path.join(download_path,'Detailed_Statistics_Departures.csv'), header = 6, skipfooter= 1, engine = 'python')\n",
    "                    bts_df['Airport'] = airport\n",
    "\n",
    "                    os.remove(os.path.join(download_path,'Detailed_Statistics_Departures.csv')) \n",
    "\n",
    "                    file_path = os.path.join(download_path,f'{airport}{year}.csv')\n",
    "\n",
    "                    bts_df.to_csv(file_path)\n",
    "\n",
    "                    yield file_path\n",
    "\n",
    "                else:\n",
    "\n",
    "                    yield file_path\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f'{airport} not in BTS database')\n",
    "\n",
    "        except:\n",
    "\n",
    "            print('driver error')\n",
    "\n",
    "            continue\n",
    "        \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_download_bts_data(selection, year, **kwargs):\n",
    "    \n",
    "    gen = download_bts_data(selection, year)\n",
    "    \n",
    "    file_list = []\n",
    "    \n",
    "    for download in range(selection + 1):\n",
    "        \n",
    "        try: \n",
    "            file_list.append(next(gen))\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "            \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bts_pickle(selection, year, download_path = r'/home/desbrium/Metis/PredictingFlightDelays/Data/BTS Departure Data', **kwargs):\n",
    "    \n",
    "    file_path = os.path.join(download_path,f'raw_bts_{selection}_airport_departures{year}.pickle')\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        airport_path_list = execute_download_bts_data(selection, year)\n",
    "\n",
    "        raw_bts_df = pd.concat([pd.read_csv(airport_path).iloc[:, 1:] for airport_path in airport_path_list], axis = 0)\n",
    "\n",
    "        with open(file_path, 'wb') as to_write:\n",
    "\n",
    "            pickle.dump(raw_bts_df, to_write)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/desbrium/Metis/PredictingFlightDelays/Data/BTS Departure Data/raw_bts_25_airport_departures2019.pickle'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bts_pickle(25, '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
